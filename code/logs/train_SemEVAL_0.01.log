processId: 24265
prarent processId: 1
{
    "train_set": "../SemEval2DocRED/train_annotated_0.01.json",
    "dev_set": "../SemEval2DocRED/dev.json",
    "test_set": "../SemEval2DocRED/test.json",
    "train_set_save": "../SemEval2DocRED/prepro_data/train_BERT.pkl",
    "dev_set_save": "../SemEval2DocRED/prepro_data/dev_BERT.pkl",
    "test_set_save": "../SemEval2DocRED/prepro_data/test_BERT.pkl",
    "checkpoint_dir": "checkpoint",
    "fig_result_dir": "fig_result",
    "model_name": "SemEVAL_0.01",
    "pretrain_model": "",
    "vocabulary_size": 200000,
    "relation_nums": 11,
    "entity_type_num": 7,
    "max_entity_num": 80,
    "word_pad": 0,
    "entity_type_pad": 0,
    "entity_id_pad": 0,
    "word_emb_size": 10,
    "pre_train_word": false,
    "data_word_vec": null,
    "finetune_word": false,
    "use_entity_type": true,
    "entity_type_size": 20,
    "use_entity_id": true,
    "entity_id_size": 20,
    "nlayers": 1,
    "lstm_hidden_size": 32,
    "lstm_dropout": 0.1,
    "lr": 0.001,
    "batch_size": 5,
    "test_batch_size": 16,
    "epoch": 300,
    "test_epoch": 10,
    "weight_decay": 0.0001,
    "negativa_alpha": 4.0,
    "log_step": 20,
    "save_model_freq": 15,
    "mention_drop": false,
    "gcn_layers": 2,
    "gcn_dim": 808,
    "dropout": 0.6,
    "activation": "relu",
    "bert_hid_size": 768,
    "bert_path": "../PLM/bert-base-uncased",
    "bert_fix": false,
    "coslr": true,
    "clip": -1,
    "k_fold": "none",
    "use_model": "bert",
    "input_theta": -1,
    "transfer_learning": false
}
Reading data from ../SemEval2DocRED/train_annotated_0.01.json.
../PLM/bert-base-uncased
loading..
finish reading ../SemEval2DocRED/train_annotated_0.01.json and save preprocessed data to ../SemEval2DocRED/prepro_data/train_BERT.pkl.
Reading data from ../SemEval2DocRED/dev.json.
../PLM/bert-base-uncased
loading..
finish reading ../SemEval2DocRED/dev.json and save preprocessed data to ../SemEval2DocRED/prepro_data/dev_BERT.pkl.
Reading data from ../SemEval2DocRED/test.json.
../PLM/bert-base-uncased
loading..
finish reading ../SemEval2DocRED/test.json and save preprocessed data to ../SemEval2DocRED/prepro_data/test_BERT.pkl.
total parameters: 216617540
2021-07-01 17:31:06.812593 training from scratch with lr 0.001
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
False
2021-07-01 17:31:16.375554 begin..
/home/ubuntu/anaconda3/envs/GAIN/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2021-07-01 17:31:20.056914 | epoch  2 | step   20 |  ms/b 22.87 | train loss 457.370 | NA acc: 1.00 | not NA acc: 0.20  | tot acc: 0.60 
2021-07-01 17:31:22.353419 | epoch  3 | step   40 |  ms/b 46.10 | train loss 134.277 | NA acc: 1.00 | not NA acc: 0.40  | tot acc: 0.70 
2021-07-01 17:31:24.627138 | epoch  4 | step   60 |  ms/b 68.17 | train loss 72.066 | NA acc: 1.00 | not NA acc: 0.87  | tot acc: 0.93 
2021-07-01 17:31:26.901107 | epoch  5 | step   80 |  ms/b 90.94 | train loss 44.597 | NA acc: 1.00 | not NA acc: 0.91  | tot acc: 0.96 
2021-07-01 17:31:29.228833 | epoch  7 | step  100 |  ms/b 22.67 | train loss 20.653 | NA acc: 1.00 | not NA acc: 1.00  | tot acc: 1.00 
2021-07-01 17:31:31.525371 | epoch  8 | step  120 |  ms/b 45.85 | train loss 13.205 | NA acc: 1.00 | not NA acc: 1.00  | tot acc: 1.00 
2021-07-01 17:31:34.164217 | epoch  9 | step  140 |  ms/b 81.52 | train loss 9.971 | NA acc: 1.00 | not NA acc: 0.98  | tot acc: 0.99 
